{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 618473 entries, 0 to 618472\n",
      "Data columns (total 20 columns):\n",
      "Date                   618473 non-null datetime64[ns]\n",
      "Store                  618473 non-null float64\n",
      "DayOfWeek              600036 non-null float64\n",
      "Sales                  600028 non-null float64\n",
      "Customers              599957 non-null float64\n",
      "Open                   599832 non-null float64\n",
      "Promo                  599791 non-null float64\n",
      "StateHoliday           599873 non-null object\n",
      "SchoolHoliday          599686 non-null float64\n",
      "StoreType              618473 non-null object\n",
      "Assortment             618473 non-null object\n",
      "CompetitionDistance    616838 non-null float64\n",
      "CompetitionOpened      618473 non-null int64\n",
      "Promo2                 618473 non-null int64\n",
      "Promo2SinceWeek        314828 non-null float64\n",
      "Promo2SinceYear        314828 non-null float64\n",
      "Promo2GoingOn          618473 non-null int64\n",
      "comp_open_since        618473 non-null float64\n",
      "DaysFromPromotion      314828 non-null float64\n",
      "Decay                  314828 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(13), int64(3), object(3)\n",
      "memory usage: 99.1+ MB\n",
      "(216185, 89)\n",
      "(216185, 89)\n",
      "(216185, 90)\n",
      "(216185, 91)\n",
      "|   iter    |  target   | colsam... |   lambd   | max_depth | subsample |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-479.1   \u001b[0m | \u001b[0m 0.7551  \u001b[0m | \u001b[0m 5.118   \u001b[0m | \u001b[0m 3.046   \u001b[0m | \u001b[0m 0.9025  \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-452.5   \u001b[0m | \u001b[95m 0.6988  \u001b[0m | \u001b[95m 5.184   \u001b[0m | \u001b[95m 4.88    \u001b[0m | \u001b[95m 0.9421  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m-448.2   \u001b[0m | \u001b[95m 0.7495  \u001b[0m | \u001b[95m 2.742   \u001b[0m | \u001b[95m 4.299   \u001b[0m | \u001b[95m 0.8794  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-499.1   \u001b[0m | \u001b[0m 0.5977  \u001b[0m | \u001b[0m 4.25    \u001b[0m | \u001b[0m 3.67    \u001b[0m | \u001b[0m 0.8345  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-485.8   \u001b[0m | \u001b[0m 0.4287  \u001b[0m | \u001b[0m 3.037   \u001b[0m | \u001b[0m 4.592   \u001b[0m | \u001b[0m 0.9296  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-466.7   \u001b[0m | \u001b[0m 0.5504  \u001b[0m | \u001b[0m 4.422   \u001b[0m | \u001b[0m 4.553   \u001b[0m | \u001b[0m 0.8503  \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m-448.0   \u001b[0m | \u001b[95m 0.7987  \u001b[0m | \u001b[95m 2.592   \u001b[0m | \u001b[95m 4.539   \u001b[0m | \u001b[95m 0.8836  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-479.8   \u001b[0m | \u001b[0m 0.7501  \u001b[0m | \u001b[0m 3.213   \u001b[0m | \u001b[0m 3.295   \u001b[0m | \u001b[0m 0.9406  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-503.0   \u001b[0m | \u001b[0m 0.5511  \u001b[0m | \u001b[0m 5.446   \u001b[0m | \u001b[0m 3.374   \u001b[0m | \u001b[0m 0.8383  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-495.1   \u001b[0m | \u001b[0m 0.6083  \u001b[0m | \u001b[0m 2.024   \u001b[0m | \u001b[0m 3.338   \u001b[0m | \u001b[0m 0.8848  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-448.2   \u001b[0m | \u001b[0m 0.7836  \u001b[0m | \u001b[0m 2.674   \u001b[0m | \u001b[0m 4.444   \u001b[0m | \u001b[0m 0.8735  \u001b[0m |\n",
      "| \u001b[95m 12      \u001b[0m | \u001b[95m-447.8   \u001b[0m | \u001b[95m 0.7677  \u001b[0m | \u001b[95m 2.691   \u001b[0m | \u001b[95m 4.347   \u001b[0m | \u001b[95m 0.8825  \u001b[0m |\n",
      "| \u001b[95m 13      \u001b[0m | \u001b[95m-447.6   \u001b[0m | \u001b[95m 0.7978  \u001b[0m | \u001b[95m 2.598   \u001b[0m | \u001b[95m 4.503   \u001b[0m | \u001b[95m 0.8914  \u001b[0m |\n",
      "=========================================================================\n",
      "{'colsample_bytree': 0.7978421458577196, 'lambd': 2.5982975135904516, 'max_depth': 4.503395071691653, 'subsample': 0.8913639174845348}\n",
      "[15:30:28] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "RMSE train: 432.316225\n",
      "RMSE CV: 492.316509\n",
      "RMSPE (CV): 6.915925%\n",
      "RMSPE (train): 6.409807%\n",
      "0             0.0\n",
      "1             0.0\n",
      "2             0.0\n",
      "3             0.0\n",
      "4             0.0\n",
      "           ...   \n",
      "637769     7259.0\n",
      "637770     9938.0\n",
      "637771    10564.0\n",
      "637772    12302.0\n",
      "637773     3913.0\n",
      "Name: Sales, Length: 637774, dtype: float64\n",
      "(637774, 9)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 618473 entries, 0 to 618472\n",
      "Data columns (total 18 columns):\n",
      "Date                   618473 non-null datetime64[ns]\n",
      "Store                  618473 non-null float64\n",
      "DayOfWeek              600036 non-null float64\n",
      "Open                   599832 non-null float64\n",
      "Promo                  599791 non-null float64\n",
      "StateHoliday           599873 non-null object\n",
      "SchoolHoliday          599686 non-null float64\n",
      "StoreType              618473 non-null object\n",
      "Assortment             618473 non-null object\n",
      "CompetitionDistance    616838 non-null float64\n",
      "CompetitionOpened      618473 non-null int64\n",
      "Promo2                 618473 non-null int64\n",
      "Promo2SinceWeek        314828 non-null float64\n",
      "Promo2SinceYear        314828 non-null float64\n",
      "Promo2GoingOn          618473 non-null int64\n",
      "comp_open_since        618473 non-null float64\n",
      "DaysFromPromotion      314828 non-null float64\n",
      "Decay                  314828 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(11), int64(3), object(3)\n",
      "memory usage: 89.7+ MB\n",
      "(277956, 87)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "DataFrame.dtypes for data must be int, float or bool.\n                Did not expect the data types in fields Date",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-89cc0f5ced38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0;31m#####calculate test statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m     \u001b[0mfinal_test_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxg_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_final_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RMSPE (test): %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrmspe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_final_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_test_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/comp1/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit, validate_features)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \"\"\"\n\u001b[1;32m    447\u001b[0m         \u001b[0;31m# pylint: disable=missing-docstring,invalid-name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0mtest_dmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0;31m# get ntree_limit to use - if none specified, default to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;31m# best_ntree_limit if defined, otherwise 0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/comp1/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[1;32m    378\u001b[0m         data, feature_names, feature_types = _maybe_pandas_data(data,\n\u001b[1;32m    379\u001b[0m                                                                 \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                                                                 feature_types)\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         data, feature_names, feature_types = _maybe_dt_data(data,\n",
      "\u001b[0;32m~/miniconda3/envs/comp1/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_maybe_pandas_data\u001b[0;34m(data, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    237\u001b[0m         msg = \"\"\"DataFrame.dtypes for data must be int, float or bool.\n\u001b[1;32m    238\u001b[0m                 Did not expect the data types in fields \"\"\"\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbad_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float or bool.\n                Did not expect the data types in fields Date"
     ]
    }
   ],
   "source": [
    "\n",
    "#%% 0. Housekeeping\n",
    "\n",
    "# =============================================================================\n",
    "# 0.1 Import packages\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "# =============================================================================\n",
    "# 0.2 Import data\n",
    "# =============================================================================\n",
    "if __name__ == '__main__':\n",
    "    train = pd.read_csv('data/train.csv', low_memory=False)\n",
    "    store = pd.read_csv('data/store.csv', low_memory=False)\n",
    "\n",
    "    #%% 1. Merging store to train data\n",
    "\n",
    "    # =============================================================================\n",
    "    # 1.1 Since data has to have the same size as 'train', a container is created\n",
    "    # =============================================================================\n",
    "\n",
    "    # Creating dataframe\n",
    "    expanded_store = train\n",
    "\n",
    "    # =============================================================================\n",
    "    # 1.2 Merging variables which do not have to be changed\n",
    "    # =============================================================================\n",
    "\n",
    "    # Variables which can be merged right away\n",
    "    easy = store.loc[:,['Store','StoreType','Assortment','CompetitionDistance']]\n",
    "\n",
    "    # Variables which can be simply merged\n",
    "    expanded_store = pd.merge(expanded_store, easy, on=['Store'])\n",
    "\n",
    "    #%% 2. Creating a dummy variable since for competition is open for every store\n",
    "\n",
    "    # =============================================================================\n",
    "    # 2.1 Checking whether there is actually competition\n",
    "    # =============================================================================\n",
    "\n",
    "    # Dataset of the variables which have to be transformed\n",
    "    competition = store.loc[:,['Store',\n",
    "                               'CompetitionOpenSinceMonth','CompetitionOpenSinceYear']]\n",
    "\n",
    "    # For easier looping\n",
    "    competition.set_index('Store', inplace=True)\n",
    "\n",
    "    # Finding stores which have competition openend\n",
    "    open_comp = competition.any(axis=1)\n",
    "\n",
    "    # Stores with competition\n",
    "    open_stores = competition.loc[open_comp].index.unique()\n",
    "\n",
    "    # =============================================================================\n",
    "    # 2.2 Generating dummy for the time competition is existing\n",
    "    # =============================================================================\n",
    "\n",
    "    # Generating Date when opened\n",
    "    for store_id in open_stores:\n",
    "        year = competition.loc[store_id,'CompetitionOpenSinceYear'].astype(int)\n",
    "        month = competition.loc[store_id,'CompetitionOpenSinceMonth'].astype(int)\n",
    "        competition.loc[store_id,'CompetitionDate'] = datetime(year,month,1)\n",
    "\n",
    "    # Creating a dummy variable for whether competition openend for each store\n",
    "    expanded_store.loc[:,'CompetitionOpened'] = 0\n",
    "\n",
    "    for store_id in open_stores:\n",
    "\n",
    "        # Getting the date when competition openend\n",
    "        date = competition.loc[store_id,'CompetitionDate']\n",
    "        date_str= date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        store_number = expanded_store.loc[:,'Store'] == store_id\n",
    "\n",
    "        maximum_date = expanded_store.loc[store_number,'Date'].max()\n",
    "\n",
    "        # Whether it falls in time frame\n",
    "        competition_existing = (expanded_store.loc[:,'Date'].between(date_str,maximum_date)) &  (expanded_store.loc[:,'Store'] == store_id)\n",
    "\n",
    "        # Indicating whether competition is around\n",
    "        expanded_store.loc[competition_existing, 'CompetitionOpened'] = 1\n",
    "\n",
    "\n",
    "    #%% 3. Creating a dummy variable for Promo\n",
    "\n",
    "    # =============================================================================\n",
    "    # 3.1 Since data has to have the same size as 'train', a container is created\n",
    "    # =============================================================================\n",
    "\n",
    "    # Create dataframe\n",
    "    promo2 = store.loc[:,['Store',\n",
    "                         'Promo2','Promo2SinceWeek','Promo2SinceYear','PromoInterval']]\n",
    "\n",
    "    # =============================================================================\n",
    "    # 3.2 Create dummy for whether a promo2 is running\n",
    "    # =============================================================================\n",
    "\n",
    "    # Getting date from which promo started\n",
    "    length = promo2.shape[0]\n",
    "    for i in range(length):\n",
    "        if promo2.loc[i,'Promo2']:\n",
    "            week = promo2.loc[i,'Promo2SinceWeek'].astype(int)\n",
    "            year = promo2.loc[i,'Promo2SinceYear'].astype(int)\n",
    "            promo2.loc[i,'promo2start'] = dt.datetime.strptime(f'{year}-W{int(week )- 1}-1', \"%Y-W%W-%w\").date()\n",
    "\n",
    "    # Merge it with the train file\n",
    "    expanded_promo = pd.merge(expanded_store, promo2, on=['Store'])\n",
    "\n",
    "\n",
    "    # Empty container with no promo indicator\n",
    "    expanded_promo.loc[:,'Promo2GoingOn'] = 0\n",
    "\n",
    "    # Month indication\n",
    "    expanded_promo.loc[:,'Date_str'] = pd.to_datetime(expanded_promo.loc[:,'Date'],).dt.strftime('%Y-%b-%d')\n",
    "    expanded_promo.loc[:,'Date'] = pd.to_datetime(expanded_promo.loc[:,'Date'],)\n",
    "    expanded_promo.loc[:,'month'] = expanded_promo.loc[:,'Date_str'].str[5:8]\n",
    "\n",
    "\n",
    "    months = expanded_promo.loc[:,'month'].unique()\n",
    "\n",
    "    for month in months:\n",
    "        month_boolean = expanded_promo.loc[:,'PromoInterval'].str.contains(month, na=False)\n",
    "        expanded_promo.loc[month_boolean,'Promo2GoingOn'] = 1\n",
    "\n",
    "\n",
    "    #%% 4. Creating time since competition opened\n",
    "\n",
    "    exp_comp = pd.merge(expanded_promo, competition, on=['Store'])\n",
    "\n",
    "    date_current = pd.to_datetime(exp_comp.loc[:,'Date'])\n",
    "    date_openend = pd.to_datetime(exp_comp.loc[:,'CompetitionDate'])\n",
    "\n",
    "    exp_comp.loc[:,'comp_open_since'] = (date_current - date_openend).astype('timedelta64[D]')\n",
    "\n",
    "    future_comp = (exp_comp.loc[:,'comp_open_since'] < 0)\n",
    "    no_comp = exp_comp.loc[:,'comp_open_since'].isna()\n",
    "\n",
    "    exp_comp.loc[future_comp, 'comp_open_since'] = 0\n",
    "    exp_comp.loc[no_comp, 'comp_open_since'] = 0\n",
    "\n",
    "    expanded_promo.loc[:,'comp_open_since'] = exp_comp.loc[:,'comp_open_since']\n",
    "\n",
    "    #%% 5. Decaying competition factor\n",
    "\n",
    "    expanded_promo.loc[:,'IntervalList'] = expanded_promo.loc[:,'PromoInterval'].str.split(pat = ',')\n",
    "\n",
    "    Interval = {'First': 0,\n",
    "    'Second' : 1,\n",
    "    'Third' : 2,\n",
    "    'Fourth': 3}\n",
    "\n",
    "    for element, value in Interval.items():\n",
    "        expanded_promo.loc[:,element] = expanded_promo.loc[:,'IntervalList'].str[value]\n",
    "\n",
    "    year = pd.to_datetime(expanded_promo.loc[:,'Date']).dt.year.astype(str)\n",
    "\n",
    "    Interval = {'Interval1': 'First',\n",
    "    'Interval2' : 'Second',\n",
    "    'Interval3' : 'Third',\n",
    "    'Interval4': 'Fourth'}\n",
    "\n",
    "    expanded_promo.loc[:,'Date_Actual'] = pd.to_datetime(expanded_promo.loc[:,'Date'])\n",
    "\n",
    "    for element, value in Interval.items():\n",
    "\n",
    "        ### New year stuff\n",
    "        expanded_promo.loc[:,'Date_Str'] = '1' + '-' + expanded_promo.loc[:,value] + '-' + year\n",
    "        dates = pd.to_datetime(expanded_promo.loc[:,'Date_Str'])\n",
    "\n",
    "        expanded_promo.loc[:,element] = (expanded_promo.loc[:,'Date_Actual'] - dates).astype('timedelta64[D]')\n",
    "\n",
    "        negative = expanded_promo.loc[:,element] < 0\n",
    "        expanded_promo.loc[negative,element] = np.nan\n",
    "\n",
    "        ### Last year stuff\n",
    "        expanded_promo.loc[:,'Date_Str'] = '1' + '-' + expanded_promo.loc[:,value] + '-' + (year.astype(int)-1).astype(str)\n",
    "        dates = pd.to_datetime(expanded_promo.loc[:,'Date_Str'])\n",
    "\n",
    "        expanded_promo.loc[:,element + 'before'] = (expanded_promo.loc[:,'Date_Actual'] - dates).astype('timedelta64[D]')\n",
    "\n",
    "        negative = expanded_promo.loc[:,element + 'before'] < 0\n",
    "        expanded_promo.loc[negative,element + 'before'] = np.nan\n",
    "\n",
    "    all_versions = expanded_promo.loc[:,['Interval1','Interval2','Interval3','Interval4',\n",
    "    'Interval1before','Interval2before','Interval3before','Interval4before',]]\n",
    "    minimum_distance = all_versions.min(axis=1, skipna=True)\n",
    "\n",
    "    expanded_promo.loc[:,'DaysFromPromotion'] = minimum_distance\n",
    "\n",
    "    expanded_promo.loc[:,'Decay'] = np.exp(- 0.05 * minimum_distance)\n",
    "\n",
    "    expanded_promo=expanded_promo[['Date', 'Store', 'DayOfWeek', 'Sales', 'Customers', 'Open', 'Promo','StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment','CompetitionDistance', 'CompetitionOpened', 'Promo2', 'Promo2SinceWeek',\n",
    "           'Promo2SinceYear', 'Promo2GoingOn', 'comp_open_since', 'DaysFromPromotion','Decay']]\n",
    "    expanded=expanded_promo.copy()\n",
    "    expanded_promo.info()\n",
    "\n",
    "    # Convert Dates to Date time\n",
    "    expanded=expanded_promo.copy()\n",
    "\n",
    "    expanded['Date']=pd.to_datetime(expanded['Date'])\n",
    "\n",
    "    #add in variables for day of month etc\n",
    "    expanded['dayofweek'] = expanded['Date'].dt.dayofweek\n",
    "    expanded['quarter'] = expanded['Date'].dt.quarter\n",
    "    expanded['month'] = expanded['Date'].dt.month\n",
    "    expanded['year'] = expanded['Date'].dt.year\n",
    "    expanded['dayofyear'] = expanded['Date'].dt.dayofyear\n",
    "    expanded['dayofmonth'] = expanded['Date'].dt.day\n",
    "    expanded['weekofyear'] = expanded['Date'].dt.weekofyear\n",
    "\n",
    "    #create dummy variables for day of week etc and categorical variables\n",
    "\n",
    "    expanded= pd.get_dummies(expanded,columns=['dayofweek','dayofmonth','quarter','month','StateHoliday','StoreType','Assortment'])\n",
    "\n",
    "    #Re-add original day of month variable etc.\n",
    "    expanded['dayofweek'] = expanded['Date'].dt.dayofweek\n",
    "    expanded['quarter'] = expanded['Date'].dt.quarter\n",
    "    expanded['month'] = expanded['Date'].dt.month\n",
    "    expanded['year'] = expanded['Date'].dt.year\n",
    "    expanded['dayofyear'] = expanded['Date'].dt.dayofyear\n",
    "    expanded['dayofmonth'] = expanded['Date'].dt.day\n",
    "    expanded['weekofyear'] = expanded['Date'].dt.weekofyear\n",
    "\n",
    "    expanded.dropna(axis = 0, how ='any',inplace=True)\n",
    "    expanded=expanded[expanded['Sales'] >0 ]\n",
    "\n",
    "    print(expanded.shape)\n",
    "\n",
    "    from datetime import timedelta\n",
    "    import datetime\n",
    "    df=expanded.copy()\n",
    "    date_range_days=(df['Date'].max() - df['Date'].min()).days\n",
    "    split_date=df['Date'].min() + timedelta(date_range_days*0.8) #train set 80% of full population\n",
    "    #randomly creating train and test subsets. may need to refine this\n",
    "    df_early,df_later = df.loc[df['Date'] <= split_date], df.loc[df['Date'] > split_date]\n",
    "    print(df.shape)\n",
    "    aggs=df_early.groupby(by=['Store']).agg({'Sales':'sum','Customers':'sum'})\n",
    "    aggs['av_SalesPerCustomer']=aggs['Sales']/aggs['Customers']\n",
    "    aggs.sort_values(by='av_SalesPerCustomer',ascending=False)\n",
    "    df_early= pd.merge(df_early, aggs['av_SalesPerCustomer'], on=['Store'])\n",
    "    df_later= pd.merge(df_later, aggs['av_SalesPerCustomer'], on=['Store'])\n",
    "    df= pd.merge(df, aggs['av_SalesPerCustomer'], on=['Store'])\n",
    "    #create sales per customer for each store and day\n",
    "    #expanded['SalesPerCustomer']=expanded['Sales']/expanded['Customers']\n",
    "    print(df.shape)\n",
    "    #create av sales per day of the week\n",
    "    aggs_1=df_early.groupby(by=['Store','dayofweek']).agg({'Sales':'sum','Customers':'sum'})\n",
    "    aggs_1['av_SalesPerCustomer_dayofweek']=aggs_1['Sales']/aggs_1['Customers']\n",
    "    aggs_1.sort_values(by='av_SalesPerCustomer_dayofweek',ascending=False)\n",
    "    #df_early= pd.merge(df_early, aggs['av_SalesPerCustomer_dayofweek'], on=['Store'])\n",
    "    #df_later= pd.merge(df_later, aggs['av_SalesPerCustomer_dayofweek'], on=['Store'])\n",
    "    df= pd.merge(df, aggs_1['av_SalesPerCustomer_dayofweek'], on=['Store','dayofweek'])\n",
    "    #create sales per customer for each store and day\n",
    "    #expanded['SalesPerCustomer']=expanded['Sales']/expanded['Customers']\n",
    "\n",
    "\n",
    "    aggs=df_early.groupby(by=['Store','dayofmonth']).agg({'Sales':'sum','Customers':'sum'})\n",
    "    aggs['av_SalesPerCustomer_dayofmonth']=aggs['Sales']/aggs['Customers']\n",
    "    aggs.sort_values(by='av_SalesPerCustomer_dayofmonth',ascending=False)\n",
    "    #df_early= pd.merge(df_early, aggs['av_SalesPerCustomer_dayofmonth'], on=['Store'])\n",
    "    #df_later= pd.merge(df_later, aggs['av_SalesPerCustomer_dayofmonth'], on=['Store'])\n",
    "    df= pd.merge(df, aggs['av_SalesPerCustomer_dayofmonth'], on=['Store','dayofmonth'])\n",
    "    #create sales per customer for each store and day\n",
    "    #expanded['SalesPerCustomer']=expanded['Sales']/expanded['Customers']\n",
    "\n",
    "    df.set_index('Date', inplace=True)\n",
    "    df_test=df.copy()\n",
    "    print(df.shape)\n",
    "\n",
    "\n",
    "\n",
    "    ######\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import xgboost as xgb\n",
    "    from xgboost import plot_importance\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.model_selection import train_test_split as train_test_split\n",
    "    pd.set_option('display.max_rows', 500)\n",
    "    pd.set_option('display.max_columns', 500)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    import datetime as dt\n",
    "    from datetime import datetime\n",
    "    from bayes_opt import BayesianOptimization\n",
    "\n",
    "\n",
    "\n",
    "    cols = list(df.columns.values) #Make a list of all of the columns in the df\n",
    "    cols.pop(cols.index('Sales')) #Remove sales from list\n",
    "    df = df[cols + ['Sales']] #Create new dataframe with sales right at the end\n",
    "    X, y = df.iloc[:, :-1],df.iloc[:, -1]\n",
    "    var_list=['av_SalesPerCustomer','av_SalesPerCustomer_dayofweek',\n",
    "                               'av_SalesPerCustomer_dayofmonth','Customers',\n",
    "           'Promo','Promo2','CompetitionDistance','dayofweek','Decay','comp_open_since','Sales']\n",
    "    df=df[var_list]\n",
    "\n",
    "    date_range_days=(df.index.max() - df.index.min()).days\n",
    "    split_date=df.index.min() + timedelta(date_range_days*0.8) #train set 80% of full population\n",
    "    #randomly creating train and test subsets. may need to refine this\n",
    "    df_early,df_later = df.loc[df.index <= split_date], df.loc[df.index > split_date]\n",
    "    #create feature matrix of everything up to sales, create labels from sales\n",
    "    X_train, X_test, y_train, y_test = df_early.iloc[:,:-1], df_later.iloc[:,:-1], df_early.iloc[:,-1], df_later.iloc[:,-1]\n",
    "\n",
    "\n",
    "\n",
    "    # creating XGB optimised data structure. we will need this for our cross validation model later\n",
    "    df_DM = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "    #here we decide the parameters that we are going to use in the model\n",
    "    params = {\"objective\":\"reg:squarederror\", #type of regressor, shouldnt change\n",
    "              'colsample_bytree': 0.627, #percentage of features used per tree. High value can lead to overfitting.\n",
    "              'learning_rate': 0.1, #step size shrinkage used to prevent overfitting. Range is [0,1]\n",
    "              'max_depth': 5, #determines how deeply each tree is allowed to grow during any boosting round. keep this low! this will blow up our variance if high\n",
    "              'lambda': 4.655, #L1 regularization on leaf weights. A large valupythone leads to more regularization. Could consider l2 euclidiean regularisation\n",
    "              'n_estimators': 1250, #number of trees you want to build.\n",
    "              'n_jobs': 4,#should optimise core usage on pc\n",
    "             'subsample':0.86}\n",
    "\n",
    "    #now we must instantiate the XGB regressor by calling XGB regressor CLASS from the XGBoost library, we must give it the hypter parameters as arguments\n",
    "    xg_reg = xgb.XGBRegressor(**params)\n",
    "    #Fit the regressor to the training set and make predictions for the test set using .fit() and .predict() methods\n",
    "    xg_reg.fit(X_train, y_train)\n",
    "    preds = xg_reg.predict(X_test)\n",
    "    preds_train = xg_reg.predict(X_train)\n",
    "\n",
    "    EPSILON = 1e-10\n",
    "    def _error(actual: np.ndarray, predicted: np.ndarray):\n",
    "        \"\"\" Simple error \"\"\"\n",
    "        return actual - predicted\n",
    "    def _percentage_error(actual: np.ndarray, predicted: np.ndarray):\n",
    "        \"\"\"\n",
    "        Percentage error\n",
    "        Note: result is NOT multiplied by 100\n",
    "        \"\"\"\n",
    "        return _error(actual, predicted) / (actual + EPSILON)\n",
    "\n",
    "    def rmspe(actual: np.ndarray, predicted: np.ndarray):\n",
    "\n",
    "        return np.sqrt(np.mean(np.square(_percentage_error(actual, predicted))))\n",
    "\n",
    "    #now we must instantiate the XGB regressor by calling XGB regressor CLASS from the XGBoost library, we must give it the hypter parameters as arguments\n",
    "    xg_reg = xgb.XGBRegressor(**params)\n",
    "\n",
    "    #Fit the regressor to the training set and make predictions for the test set using .fit() and .predict() methods\n",
    "    xg_reg.fit(X_train, y_train)\n",
    "    test_preds = xg_reg.predict(X_test)\n",
    "    train_preds = xg_reg.predict(X_train)\n",
    "    #print(\"RMSE train: %f\" % np.sqrt(mean_squared_error(y_train, train_preds)))\n",
    "    #print(\"RMSE test: %f\" % np.sqrt(mean_squared_error(y_test, test_preds)))\n",
    "\n",
    "    #print(\"RMSPE (test): %f\" % (rmspe(y_test,preds)*100) +'%')\n",
    "    #print(\"RMSPE (train): %f\" % (rmspe(y_train,preds_train)*100) +'%')\n",
    "    #print(np.sqrt(mean_squared_error(y_test, preds)))\n",
    "    #logger.append(X_train.columns)\n",
    "    #logger.append(rmspe(y_test,preds)*100)\n",
    "\n",
    "     #bayesian optimisation of hyper parameters\n",
    "    def xgb_evaluate(max_depth, lambd, colsample_bytree,subsample):\n",
    "        params1 = {'objective': 'reg:squarederror',\n",
    "                  'max_depth': int(max_depth),\n",
    "                  'learning_rate': 0.1,\n",
    "                  'lambda': lambd,\n",
    "                   'subsample': subsample,\n",
    "                  'colsample_bytree': colsample_bytree,\n",
    "                  'n_jobs': 4}\n",
    "        # Used around 1000 boosting rounds in the full model\n",
    "        cv_result = xgb.cv(dtrain=df_DM, params=params1, num_boost_round=125, nfold=3,metrics='rmse',seed=42)    \n",
    "\n",
    "        # Bayesian optimization only knows how to maximize, not minimize, so return the negative RMSE\n",
    "        return -1.0 * cv_result['test-rmse-mean'].iloc[-1]\n",
    "\n",
    "    xgb_bo = BayesianOptimization(xgb_evaluate, {'max_depth': (3, 5), \n",
    "                                                 'lambd': (2, 6),\n",
    "                                                 'colsample_bytree': (0.3, 0.8),\n",
    "                                                'subsample': (0.8,1)})\n",
    "    # Use the expected improvement acquisition function to handle negative numbers\n",
    "    # Optimally needs quite a few more initiation points and number of iterations\n",
    "    xgb_bo.maximize(init_points=10, n_iter=3, acq='ei')\n",
    "    #extract best parameters from model\n",
    "    params1 = xgb_bo.max['params']\n",
    "    print (params1)\n",
    "    #Converting the max_depth and from float to int\n",
    "    params1['max_depth']= int(params1['max_depth'])\n",
    "    \n",
    "    xg_reg2 = xgb.XGBRegressor(**params1,n_estimators=500)\n",
    "    xg_reg2.fit(X_train, y_train)\n",
    "    train_preds1 = xg_reg2.predict(X_train)\n",
    "    test_preds1 = xg_reg2.predict(X_test)\n",
    "    print(\"RMSE train: %f\" % np.sqrt(mean_squared_error(y_train, train_preds1)))\n",
    "    print(\"RMSE CV: %f\" % np.sqrt(mean_squared_error(y_test, test_preds1)))\n",
    "    print(\"RMSPE (CV): %f\" % (rmspe(y_test,test_preds1)*100) +'%')\n",
    "    print(\"RMSPE (train): %f\" % (rmspe(y_train,train_preds1)*100) +'%')\n",
    " \n",
    "\n",
    "    \n",
    "    ######end of model \n",
    "\n",
    "    test = pd.read_csv('data/test.csv', low_memory=False)\n",
    "    store = pd.read_csv('data/store.csv', low_memory=False)\n",
    "        \n",
    "    y_final_test=test['Sales']\n",
    "    test.drop(columns=['Sales'])\n",
    "    \n",
    "    print(y_final_test)\n",
    "    print(test.shape)\n",
    "    \n",
    "    #%% 1. Merging store to train data\n",
    "\n",
    "    # =============================================================================\n",
    "    # 1.1 Since data has to have the same size as 'train', a container is created\n",
    "    # =============================================================================\n",
    "\n",
    "    # Creating dataframe\n",
    "    expanded_store = test\n",
    "\n",
    "    # =============================================================================\n",
    "    # 1.2 Merging variables which do not have to be changed\n",
    "    # =============================================================================\n",
    "\n",
    "    # Variables which can be merged right away\n",
    "    easy = store.loc[:,['Store','StoreType','Assortment','CompetitionDistance']]\n",
    "\n",
    "    # Variables which can be simply merged\n",
    "    expanded_store = pd.merge(expanded_store, easy, on=['Store'])\n",
    "\n",
    "    #%% 2. Creating a dummy variable since for competition is open for every store\n",
    "\n",
    "    # =============================================================================\n",
    "    # 2.1 Checking whether there is actually competition\n",
    "    # =============================================================================\n",
    "\n",
    "    # Dataset of the variables which have to be transformed\n",
    "    competition = store.loc[:,['Store',\n",
    "                               'CompetitionOpenSinceMonth','CompetitionOpenSinceYear']]\n",
    "\n",
    "    # For easier looping\n",
    "    competition.set_index('Store', inplace=True)\n",
    "\n",
    "    # Finding stores which have competition openend\n",
    "    open_comp = competition.any(axis=1)\n",
    "\n",
    "    # Stores with competition\n",
    "    open_stores = competition.loc[open_comp].index.unique()\n",
    "\n",
    "    # =============================================================================\n",
    "    # 2.2 Generating dummy for the time competition is existing\n",
    "    # =============================================================================\n",
    "\n",
    "    # Generating Date when opened\n",
    "    for store_id in open_stores:\n",
    "        year = competition.loc[store_id,'CompetitionOpenSinceYear'].astype(int)\n",
    "        month = competition.loc[store_id,'CompetitionOpenSinceMonth'].astype(int)\n",
    "        competition.loc[store_id,'CompetitionDate'] = datetime(year,month,1)\n",
    "\n",
    "    # Creating a dummy variable for whether competition openend for each store\n",
    "    expanded_store.loc[:,'CompetitionOpened'] = 0\n",
    "\n",
    "    for store_id in open_stores:\n",
    "\n",
    "        # Getting the date when competition openend\n",
    "        date = competition.loc[store_id,'CompetitionDate']\n",
    "        date_str= date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        store_number = expanded_store.loc[:,'Store'] == store_id\n",
    "\n",
    "        maximum_date = expanded_store.loc[store_number,'Date'].max()\n",
    "\n",
    "        # Whether it falls in time frame\n",
    "        competition_existing = (expanded_store.loc[:,'Date'].between(date_str,maximum_date)) &  (expanded_store.loc[:,'Store'] == store_id)\n",
    "\n",
    "        # Indicating whether competition is around\n",
    "        expanded_store.loc[competition_existing, 'CompetitionOpened'] = 1\n",
    "\n",
    "\n",
    "    #%% 3. Creating a dummy variable for Promo\n",
    "\n",
    "    # =============================================================================\n",
    "    # 3.1 Since data has to have the same size as 'train', a container is created\n",
    "    # =============================================================================\n",
    "\n",
    "    # Create dataframe\n",
    "    promo2 = store.loc[:,['Store',\n",
    "                         'Promo2','Promo2SinceWeek','Promo2SinceYear','PromoInterval']]\n",
    "\n",
    "    # =============================================================================\n",
    "    # 3.2 Create dummy for whether a promo2 is running\n",
    "    # =============================================================================\n",
    "\n",
    "    # Getting date from which promo started\n",
    "    length = promo2.shape[0]\n",
    "    for i in range(length):\n",
    "        if promo2.loc[i,'Promo2']:\n",
    "            week = promo2.loc[i,'Promo2SinceWeek'].astype(int)\n",
    "            year = promo2.loc[i,'Promo2SinceYear'].astype(int)\n",
    "            promo2.loc[i,'promo2start'] = dt.datetime.strptime(f'{year}-W{int(week )- 1}-1', \"%Y-W%W-%w\").date()\n",
    "\n",
    "    # Merge it with the train file\n",
    "    expanded_promo = pd.merge(expanded_store, promo2, on=['Store'])\n",
    "\n",
    "\n",
    "    # Empty container with no promo indicator\n",
    "    expanded_promo.loc[:,'Promo2GoingOn'] = 0\n",
    "\n",
    "    # Month indication\n",
    "    expanded_promo.loc[:,'Date_str'] = pd.to_datetime(expanded_promo.loc[:,'Date'],).dt.strftime('%Y-%b-%d')\n",
    "    expanded_promo.loc[:,'Date'] = pd.to_datetime(expanded_promo.loc[:,'Date'],)\n",
    "    expanded_promo.loc[:,'month'] = expanded_promo.loc[:,'Date_str'].str[5:8]\n",
    "\n",
    "\n",
    "    months = expanded_promo.loc[:,'month'].unique()\n",
    "\n",
    "    for month in months:\n",
    "        month_boolean = expanded_promo.loc[:,'PromoInterval'].str.contains(month, na=False)\n",
    "        expanded_promo.loc[month_boolean,'Promo2GoingOn'] = 1\n",
    "\n",
    "\n",
    "    #%% 4. Creating time since competition opened\n",
    "\n",
    "    exp_comp = pd.merge(expanded_promo, competition, on=['Store'])\n",
    "\n",
    "    date_current = pd.to_datetime(exp_comp.loc[:,'Date'])\n",
    "    date_openend = pd.to_datetime(exp_comp.loc[:,'CompetitionDate'])\n",
    "\n",
    "    exp_comp.loc[:,'comp_open_since'] = (date_current - date_openend).astype('timedelta64[D]')\n",
    "\n",
    "    future_comp = (exp_comp.loc[:,'comp_open_since'] < 0)\n",
    "    no_comp = exp_comp.loc[:,'comp_open_since'].isna()\n",
    "\n",
    "    exp_comp.loc[future_comp, 'comp_open_since'] = 0\n",
    "    exp_comp.loc[no_comp, 'comp_open_since'] = 0\n",
    "\n",
    "    expanded_promo.loc[:,'comp_open_since'] = exp_comp.loc[:,'comp_open_since']\n",
    "\n",
    "    #%% 5. Decaying competition factor\n",
    "\n",
    "    expanded_promo.loc[:,'IntervalList'] = expanded_promo.loc[:,'PromoInterval'].str.split(pat = ',')\n",
    "\n",
    "    Interval = {'First': 0,\n",
    "    'Second' : 1,\n",
    "    'Third' : 2,\n",
    "    'Fourth': 3}\n",
    "\n",
    "    for element, value in Interval.items():\n",
    "        expanded_promo.loc[:,element] = expanded_promo.loc[:,'IntervalList'].str[value]\n",
    "\n",
    "    year = pd.to_datetime(expanded_promo.loc[:,'Date']).dt.year.astype(str)\n",
    "\n",
    "    Interval = {'Interval1': 'First',\n",
    "    'Interval2' : 'Second',\n",
    "    'Interval3' : 'Third',\n",
    "    'Interval4': 'Fourth'}\n",
    "\n",
    "    expanded_promo.loc[:,'Date_Actual'] = pd.to_datetime(expanded_promo.loc[:,'Date'])\n",
    "\n",
    "    for element, value in Interval.items():\n",
    "\n",
    "        ### New year stuff\n",
    "        expanded_promo.loc[:,'Date_Str'] = '1' + '-' + expanded_promo.loc[:,value] + '-' + year\n",
    "        dates = pd.to_datetime(expanded_promo.loc[:,'Date_Str'])\n",
    "\n",
    "        expanded_promo.loc[:,element] = (expanded_promo.loc[:,'Date_Actual'] - dates).astype('timedelta64[D]')\n",
    "\n",
    "        negative = expanded_promo.loc[:,element] < 0\n",
    "        expanded_promo.loc[negative,element] = np.nan\n",
    "\n",
    "        ### Last year stuff\n",
    "        expanded_promo.loc[:,'Date_Str'] = '1' + '-' + expanded_promo.loc[:,value] + '-' + (year.astype(int)-1).astype(str)\n",
    "        dates = pd.to_datetime(expanded_promo.loc[:,'Date_Str'])\n",
    "\n",
    "        expanded_promo.loc[:,element + 'before'] = (expanded_promo.loc[:,'Date_Actual'] - dates).astype('timedelta64[D]')\n",
    "\n",
    "        negative = expanded_promo.loc[:,element + 'before'] < 0\n",
    "        expanded_promo.loc[negative,element + 'before'] = np.nan\n",
    "\n",
    "    all_versions = expanded_promo.loc[:,['Interval1','Interval2','Interval3','Interval4',\n",
    "                                         'Interval1before','Interval2before','Interval3before','Interval4before',]]\n",
    "    minimum_distance = all_versions.min(axis=1, skipna=True)\n",
    "\n",
    "    expanded_promo.loc[:,'DaysFromPromotion'] = minimum_distance\n",
    "\n",
    "    expanded_promo.loc[:,'Decay'] = np.exp(- 0.05 * minimum_distance)\n",
    "\n",
    "    expanded_promo=expanded_promo[['Date', 'Store', 'DayOfWeek', 'Open', 'Promo','StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment','CompetitionDistance', 'CompetitionOpened', 'Promo2', 'Promo2SinceWeek',\n",
    "           'Promo2SinceYear', 'Promo2GoingOn', 'comp_open_since', 'DaysFromPromotion','Decay']]\n",
    "    expanded=expanded_promo.copy()\n",
    "    expanded_promo.info()\n",
    "\n",
    "    #%% Rachel - General stuff\n",
    "\n",
    "    # Convert Dates to Date time\n",
    "    expanded=expanded_promo.copy()\n",
    "\n",
    "    expanded['Date']=pd.to_datetime(expanded['Date'])\n",
    "\n",
    "    #add in variables for day of month etc\n",
    "    expanded['dayofweek'] = expanded['Date'].dt.dayofweek\n",
    "    expanded['quarter'] = expanded['Date'].dt.quarter\n",
    "    expanded['month'] = expanded['Date'].dt.month\n",
    "    expanded['year'] = expanded['Date'].dt.year\n",
    "    expanded['dayofyear'] = expanded['Date'].dt.dayofyear\n",
    "    expanded['dayofmonth'] = expanded['Date'].dt.day\n",
    "    expanded['weekofyear'] = expanded['Date'].dt.weekofyear\n",
    "\n",
    "    #create dummy variables for day of week etc and categorical variables\n",
    "\n",
    "    expanded= pd.get_dummies(expanded,columns=['dayofweek','dayofmonth','quarter','month','StateHoliday','StoreType','Assortment'])\n",
    "\n",
    "    #Re-add original day of month variable etc.\n",
    "    expanded['dayofweek'] = expanded['Date'].dt.dayofweek\n",
    "    expanded['quarter'] = expanded['Date'].dt.quarter\n",
    "    expanded['month'] = expanded['Date'].dt.month\n",
    "    expanded['year'] = expanded['Date'].dt.year\n",
    "    expanded['dayofyear'] = expanded['Date'].dt.dayofyear\n",
    "    expanded['dayofmonth'] = expanded['Date'].dt.day\n",
    "    expanded['weekofyear'] = expanded['Date'].dt.weekofyear\n",
    "\n",
    "    expanded.dropna(axis = 0, how ='any',inplace=True)\n",
    "\n",
    "    print(expanded.shape)\n",
    "\n",
    "    #%% Rachel - Merge on relevant variables\n",
    "\n",
    "    # =============================================================================\n",
    "    # Extracting relevant variables with the relevant merge key variables\n",
    "    # =============================================================================\n",
    "\n",
    "    SalesPerCustomer_df = df_test.loc[:,['av_SalesPerCustomer','Store']].drop_duplicates()\n",
    "    SalesPerCustomer_dayW  = df_test.loc[:,['av_SalesPerCustomer_dayofweek','Store','dayofweek']].drop_duplicates()\n",
    "    SalesPerCustomer_dayM = df_test.loc[:,['av_SalesPerCustomer_dayofmonth','Store','dayofmonth']].drop_duplicates()\n",
    "\n",
    "    # =============================================================================\n",
    "    # Merging the relevant variables to the main dataset\n",
    "    # =============================================================================\n",
    "\n",
    "    test_merge = pd.merge(expanded,SalesPerCustomer_df, on=['Store'], validate='many_to_one')\n",
    "    test_merge = pd.merge(test_merge, SalesPerCustomer_dayW, on=['Store','dayofweek'], validate='many_to_one')\n",
    "    test_merge = pd.merge(test_merge, SalesPerCustomer_dayM, on=['Store','dayofmonth'], validate='many_to_one')\n",
    "\n",
    "\n",
    "\n",
    "    test_merge.columns\n",
    "    test_columns=['av_SalesPerCustomer','av_SalesPerCustomer_dayofweek',\n",
    "                               'av_SalesPerCustomer_dayofmonth','Customers',\n",
    "           'Promo','Promo2','CompetitionDistance','dayofweek','Decay','comp_open_since']\n",
    "    test_merge=test_merge[test_columns]\n",
    "    X_final_test=test_merge.copy()\n",
    "\n",
    "    #####calculate test statistics\n",
    "    final_test_preds = xg_reg.predict(X_final_test)\n",
    "    print(\"RMSPE (test): %f\" % (rmspe(y_final_test,final_test_preds)*100) +'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>av_SalesPerCustomer</th>\n",
       "      <th>Store</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>14.296748</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>7.836269</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>11.901118</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>13.906036</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>11.155245</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            av_SalesPerCustomer  Store\n",
       "Date                                  \n",
       "2013-01-02            14.296748    NaN\n",
       "2013-01-03             7.836269    NaN\n",
       "2013-01-02            11.901118    NaN\n",
       "2013-01-02            13.906036    NaN\n",
       "2013-01-02            11.155245    NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SalesPerCustomer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['av_SalesPerCustomer', 'av_SalesPerCustomer_dayofweek', 'av_SalesPerCustomer_dayofmonth', 'Customers', 'Promo', 'Promo2', 'CompetitionDistance', 'dayofweek', 'Decay', 'comp_open_since', 'Sales'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'reg:squarederror',\n",
       " 'colsample_bytree': 0.627,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': 5,\n",
       " 'lambda': 4.655,\n",
       " 'n_estimators': 1250,\n",
       " 'n_jobs': 4,\n",
       " 'subsample': 0.86}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 618473 entries, 0 to 618472\n",
      "Data columns (total 20 columns):\n",
      "Date                   618473 non-null datetime64[ns]\n",
      "Store                  618473 non-null float64\n",
      "DayOfWeek              600036 non-null float64\n",
      "Sales                  600028 non-null float64\n",
      "Customers              599957 non-null float64\n",
      "Open                   599832 non-null float64\n",
      "Promo                  599791 non-null float64\n",
      "StateHoliday           599873 non-null object\n",
      "SchoolHoliday          599686 non-null float64\n",
      "StoreType              618473 non-null object\n",
      "Assortment             618473 non-null object\n",
      "CompetitionDistance    616838 non-null float64\n",
      "CompetitionOpened      618473 non-null int64\n",
      "Promo2                 618473 non-null int64\n",
      "Promo2SinceWeek        314828 non-null float64\n",
      "Promo2SinceYear        314828 non-null float64\n",
      "Promo2GoingOn          618473 non-null int64\n",
      "comp_open_since        618473 non-null float64\n",
      "DaysFromPromotion      314828 non-null float64\n",
      "Decay                  314828 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(13), int64(3), object(3)\n",
      "memory usage: 99.1+ MB\n",
      "(216185, 89)\n",
      "(216185, 89)\n",
      "(216185, 90)\n",
      "(216185, 91)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 618473 entries, 0 to 618472\n",
      "Data columns (total 20 columns):\n",
      "Date                   618473 non-null datetime64[ns]\n",
      "Store                  618473 non-null float64\n",
      "Customers              599957 non-null float64\n",
      "DayOfWeek              600036 non-null float64\n",
      "Open                   599832 non-null float64\n",
      "Promo                  599791 non-null float64\n",
      "StateHoliday           599873 non-null object\n",
      "SchoolHoliday          599686 non-null float64\n",
      "StoreType              618473 non-null object\n",
      "Assortment             618473 non-null object\n",
      "CompetitionDistance    616838 non-null float64\n",
      "CompetitionOpened      618473 non-null int64\n",
      "Promo2                 618473 non-null int64\n",
      "Promo2SinceWeek        314828 non-null float64\n",
      "Promo2SinceYear        314828 non-null float64\n",
      "Promo2GoingOn          618473 non-null int64\n",
      "comp_open_since        618473 non-null float64\n",
      "Sales                  600028 non-null float64\n",
      "DaysFromPromotion      314828 non-null float64\n",
      "Decay                  314828 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(13), int64(3), object(3)\n",
      "memory usage: 99.1+ MB\n",
      "(216185, 89)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#%% 0. Housekeeping\n",
    "\n",
    "# =============================================================================\n",
    "# 0.1 Import packages\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "# =============================================================================\n",
    "# 0.2 Import data\n",
    "# =============================================================================\n",
    "if __name__ == '__main__':\n",
    "    train = pd.read_csv('data/train.csv', low_memory=False)\n",
    "    store = pd.read_csv('data/store.csv', low_memory=False)\n",
    "\n",
    "    #%% 1. Merging store to train data\n",
    "\n",
    "    # =============================================================================\n",
    "    # 1.1 Since data has to have the same size as 'train', a container is created\n",
    "    # =============================================================================\n",
    "\n",
    "    # Creating dataframe\n",
    "    expanded_store = train\n",
    "\n",
    "    # =============================================================================\n",
    "    # 1.2 Merging variables which do not have to be changed\n",
    "    # =============================================================================\n",
    "\n",
    "    # Variables which can be merged right away\n",
    "    easy = store.loc[:,['Store','StoreType','Assortment','CompetitionDistance']]\n",
    "\n",
    "    # Variables which can be simply merged\n",
    "    expanded_store = pd.merge(expanded_store, easy, on=['Store'])\n",
    "\n",
    "    #%% 2. Creating a dummy variable since for competition is open for every store\n",
    "\n",
    "    # =============================================================================\n",
    "    # 2.1 Checking whether there is actually competition\n",
    "    # =============================================================================\n",
    "\n",
    "    # Dataset of the variables which have to be transformed\n",
    "    competition = store.loc[:,['Store',\n",
    "                               'CompetitionOpenSinceMonth','CompetitionOpenSinceYear']]\n",
    "\n",
    "    # For easier looping\n",
    "    competition.set_index('Store', inplace=True)\n",
    "\n",
    "    # Finding stores which have competition openend\n",
    "    open_comp = competition.any(axis=1)\n",
    "\n",
    "    # Stores with competition\n",
    "    open_stores = competition.loc[open_comp].index.unique()\n",
    "\n",
    "    # =============================================================================\n",
    "    # 2.2 Generating dummy for the time competition is existing\n",
    "    # =============================================================================\n",
    "\n",
    "    # Generating Date when opened\n",
    "    for store_id in open_stores:\n",
    "        year = competition.loc[store_id,'CompetitionOpenSinceYear'].astype(int)\n",
    "        month = competition.loc[store_id,'CompetitionOpenSinceMonth'].astype(int)\n",
    "        competition.loc[store_id,'CompetitionDate'] = datetime(year,month,1)\n",
    "\n",
    "    # Creating a dummy variable for whether competition openend for each store\n",
    "    expanded_store.loc[:,'CompetitionOpened'] = 0\n",
    "\n",
    "    for store_id in open_stores:\n",
    "\n",
    "        # Getting the date when competition openend\n",
    "        date = competition.loc[store_id,'CompetitionDate']\n",
    "        date_str= date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        store_number = expanded_store.loc[:,'Store'] == store_id\n",
    "\n",
    "        maximum_date = expanded_store.loc[store_number,'Date'].max()\n",
    "\n",
    "        # Whether it falls in time frame\n",
    "        competition_existing = (expanded_store.loc[:,'Date'].between(date_str,maximum_date)) &  (expanded_store.loc[:,'Store'] == store_id)\n",
    "\n",
    "        # Indicating whether competition is around\n",
    "        expanded_store.loc[competition_existing, 'CompetitionOpened'] = 1\n",
    "\n",
    "\n",
    "    #%% 3. Creating a dummy variable for Promo\n",
    "\n",
    "    # =============================================================================\n",
    "    # 3.1 Since data has to have the same size as 'train', a container is created\n",
    "    # =============================================================================\n",
    "\n",
    "    # Create dataframe\n",
    "    promo2 = store.loc[:,['Store',\n",
    "                         'Promo2','Promo2SinceWeek','Promo2SinceYear','PromoInterval']]\n",
    "\n",
    "    # =============================================================================\n",
    "    # 3.2 Create dummy for whether a promo2 is running\n",
    "    # =============================================================================\n",
    "\n",
    "    # Getting date from which promo started\n",
    "    length = promo2.shape[0]\n",
    "    for i in range(length):\n",
    "        if promo2.loc[i,'Promo2']:\n",
    "            week = promo2.loc[i,'Promo2SinceWeek'].astype(int)\n",
    "            year = promo2.loc[i,'Promo2SinceYear'].astype(int)\n",
    "            promo2.loc[i,'promo2start'] = dt.datetime.strptime(f'{year}-W{int(week )- 1}-1', \"%Y-W%W-%w\").date()\n",
    "\n",
    "    # Merge it with the train file\n",
    "    expanded_promo = pd.merge(expanded_store, promo2, on=['Store'])\n",
    "\n",
    "\n",
    "    # Empty container with no promo indicator\n",
    "    expanded_promo.loc[:,'Promo2GoingOn'] = 0\n",
    "\n",
    "    # Month indication\n",
    "    expanded_promo.loc[:,'Date_str'] = pd.to_datetime(expanded_promo.loc[:,'Date'],).dt.strftime('%Y-%b-%d')\n",
    "    expanded_promo.loc[:,'Date'] = pd.to_datetime(expanded_promo.loc[:,'Date'],)\n",
    "    expanded_promo.loc[:,'month'] = expanded_promo.loc[:,'Date_str'].str[5:8]\n",
    "\n",
    "\n",
    "    months = expanded_promo.loc[:,'month'].unique()\n",
    "\n",
    "    for month in months:\n",
    "        month_boolean = expanded_promo.loc[:,'PromoInterval'].str.contains(month, na=False)\n",
    "        expanded_promo.loc[month_boolean,'Promo2GoingOn'] = 1\n",
    "\n",
    "\n",
    "    #%% 4. Creating time since competition opened\n",
    "\n",
    "    exp_comp = pd.merge(expanded_promo, competition, on=['Store'])\n",
    "\n",
    "    date_current = pd.to_datetime(exp_comp.loc[:,'Date'])\n",
    "    date_openend = pd.to_datetime(exp_comp.loc[:,'CompetitionDate'])\n",
    "\n",
    "    exp_comp.loc[:,'comp_open_since'] = (date_current - date_openend).astype('timedelta64[D]')\n",
    "\n",
    "    future_comp = (exp_comp.loc[:,'comp_open_since'] < 0)\n",
    "    no_comp = exp_comp.loc[:,'comp_open_since'].isna()\n",
    "\n",
    "    exp_comp.loc[future_comp, 'comp_open_since'] = 0\n",
    "    exp_comp.loc[no_comp, 'comp_open_since'] = 0\n",
    "\n",
    "    expanded_promo.loc[:,'comp_open_since'] = exp_comp.loc[:,'comp_open_since']\n",
    "\n",
    "    #%% 5. Decaying competition factor\n",
    "\n",
    "    expanded_promo.loc[:,'IntervalList'] = expanded_promo.loc[:,'PromoInterval'].str.split(pat = ',')\n",
    "\n",
    "    Interval = {'First': 0,\n",
    "    'Second' : 1,\n",
    "    'Third' : 2,\n",
    "    'Fourth': 3}\n",
    "\n",
    "    for element, value in Interval.items():\n",
    "        expanded_promo.loc[:,element] = expanded_promo.loc[:,'IntervalList'].str[value]\n",
    "\n",
    "    year = pd.to_datetime(expanded_promo.loc[:,'Date']).dt.year.astype(str)\n",
    "\n",
    "    Interval = {'Interval1': 'First',\n",
    "    'Interval2' : 'Second',\n",
    "    'Interval3' : 'Third',\n",
    "    'Interval4': 'Fourth'}\n",
    "\n",
    "    expanded_promo.loc[:,'Date_Actual'] = pd.to_datetime(expanded_promo.loc[:,'Date'])\n",
    "\n",
    "    for element, value in Interval.items():\n",
    "\n",
    "        ### New year stuff\n",
    "        expanded_promo.loc[:,'Date_Str'] = '1' + '-' + expanded_promo.loc[:,value] + '-' + year\n",
    "        dates = pd.to_datetime(expanded_promo.loc[:,'Date_Str'])\n",
    "\n",
    "        expanded_promo.loc[:,element] = (expanded_promo.loc[:,'Date_Actual'] - dates).astype('timedelta64[D]')\n",
    "\n",
    "        negative = expanded_promo.loc[:,element] < 0\n",
    "        expanded_promo.loc[negative,element] = np.nan\n",
    "\n",
    "        ### Last year stuff\n",
    "        expanded_promo.loc[:,'Date_Str'] = '1' + '-' + expanded_promo.loc[:,value] + '-' + (year.astype(int)-1).astype(str)\n",
    "        dates = pd.to_datetime(expanded_promo.loc[:,'Date_Str'])\n",
    "\n",
    "        expanded_promo.loc[:,element + 'before'] = (expanded_promo.loc[:,'Date_Actual'] - dates).astype('timedelta64[D]')\n",
    "\n",
    "        negative = expanded_promo.loc[:,element + 'before'] < 0\n",
    "        expanded_promo.loc[negative,element + 'before'] = np.nan\n",
    "\n",
    "    all_versions = expanded_promo.loc[:,['Interval1','Interval2','Interval3','Interval4',\n",
    "    'Interval1before','Interval2before','Interval3before','Interval4before',]]\n",
    "    minimum_distance = all_versions.min(axis=1, skipna=True)\n",
    "\n",
    "    expanded_promo.loc[:,'DaysFromPromotion'] = minimum_distance\n",
    "\n",
    "    expanded_promo.loc[:,'Decay'] = np.exp(- 0.05 * minimum_distance)\n",
    "\n",
    "    expanded_promo=expanded_promo[['Date', 'Store', 'DayOfWeek', 'Sales', 'Customers', 'Open', 'Promo','StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment','CompetitionDistance', 'CompetitionOpened', 'Promo2', 'Promo2SinceWeek',\n",
    "           'Promo2SinceYear', 'Promo2GoingOn', 'comp_open_since', 'DaysFromPromotion','Decay']]\n",
    "    expanded=expanded_promo.copy()\n",
    "    expanded_promo.info()\n",
    "\n",
    "    # Convert Dates to Date time\n",
    "    expanded=expanded_promo.copy()\n",
    "\n",
    "    expanded['Date']=pd.to_datetime(expanded['Date'])\n",
    "\n",
    "    #add in variables for day of month etc\n",
    "    expanded['dayofweek'] = expanded['Date'].dt.dayofweek\n",
    "    expanded['quarter'] = expanded['Date'].dt.quarter\n",
    "    expanded['month'] = expanded['Date'].dt.month\n",
    "    expanded['year'] = expanded['Date'].dt.year\n",
    "    expanded['dayofyear'] = expanded['Date'].dt.dayofyear\n",
    "    expanded['dayofmonth'] = expanded['Date'].dt.day\n",
    "    expanded['weekofyear'] = expanded['Date'].dt.weekofyear\n",
    "\n",
    "    #create dummy variables for day of week etc and categorical variables\n",
    "\n",
    "    expanded= pd.get_dummies(expanded,columns=['dayofweek','dayofmonth','quarter','month','StateHoliday','StoreType','Assortment'])\n",
    "\n",
    "    #Re-add original day of month variable etc.\n",
    "    expanded['dayofweek'] = expanded['Date'].dt.dayofweek\n",
    "    expanded['quarter'] = expanded['Date'].dt.quarter\n",
    "    expanded['month'] = expanded['Date'].dt.month\n",
    "    expanded['year'] = expanded['Date'].dt.year\n",
    "    expanded['dayofyear'] = expanded['Date'].dt.dayofyear\n",
    "    expanded['dayofmonth'] = expanded['Date'].dt.day\n",
    "    expanded['weekofyear'] = expanded['Date'].dt.weekofyear\n",
    "\n",
    "    expanded.dropna(axis = 0, how ='any',inplace=True)\n",
    "    expanded=expanded[expanded['Sales'] >0 ]\n",
    "\n",
    "    print(expanded.shape)\n",
    "\n",
    "    from datetime import timedelta\n",
    "    import datetime\n",
    "    df=expanded.copy()\n",
    "    date_range_days=(df['Date'].max() - df['Date'].min()).days\n",
    "    split_date=df['Date'].min() + timedelta(date_range_days*0.8) #train set 80% of full population\n",
    "    #randomly creating train and test subsets. may need to refine this\n",
    "    df_early,df_later = df.loc[df['Date'] <= split_date], df.loc[df['Date'] > split_date]\n",
    "    print(df.shape)\n",
    "    aggs=df_early.groupby(by=['Store']).agg({'Sales':'sum','Customers':'sum'})\n",
    "    aggs['av_SalesPerCustomer']=aggs['Sales']/aggs['Customers']\n",
    "    aggs.sort_values(by='av_SalesPerCustomer',ascending=False)\n",
    "    df_early= pd.merge(df_early, aggs['av_SalesPerCustomer'], on=['Store'])\n",
    "    df_later= pd.merge(df_later, aggs['av_SalesPerCustomer'], on=['Store'])\n",
    "    df= pd.merge(df, aggs['av_SalesPerCustomer'], on=['Store'])\n",
    "    #create sales per customer for each store and day\n",
    "    #expanded['SalesPerCustomer']=expanded['Sales']/expanded['Customers']\n",
    "    print(df.shape)\n",
    "    #create av sales per day of the week\n",
    "    aggs_1=df_early.groupby(by=['Store','dayofweek']).agg({'Sales':'sum','Customers':'sum'})\n",
    "    aggs_1['av_SalesPerCustomer_dayofweek']=aggs_1['Sales']/aggs_1['Customers']\n",
    "    aggs_1.sort_values(by='av_SalesPerCustomer_dayofweek',ascending=False)\n",
    "    #df_early= pd.merge(df_early, aggs['av_SalesPerCustomer_dayofweek'], on=['Store'])\n",
    "    #df_later= pd.merge(df_later, aggs['av_SalesPerCustomer_dayofweek'], on=['Store'])\n",
    "    df= pd.merge(df, aggs_1['av_SalesPerCustomer_dayofweek'], on=['Store','dayofweek'])\n",
    "    #create sales per customer for each store and day\n",
    "    #expanded['SalesPerCustomer']=expanded['Sales']/expanded['Customers']\n",
    "\n",
    "\n",
    "    aggs=df_early.groupby(by=['Store','dayofmonth']).agg({'Sales':'sum','Customers':'sum'})\n",
    "    aggs['av_SalesPerCustomer_dayofmonth']=aggs['Sales']/aggs['Customers']\n",
    "    aggs.sort_values(by='av_SalesPerCustomer_dayofmonth',ascending=False)\n",
    "    #df_early= pd.merge(df_early, aggs['av_SalesPerCustomer_dayofmonth'], on=['Store'])\n",
    "    #df_later= pd.merge(df_later, aggs['av_SalesPerCustomer_dayofmonth'], on=['Store'])\n",
    "    df= pd.merge(df, aggs['av_SalesPerCustomer_dayofmonth'], on=['Store','dayofmonth'])\n",
    "    #create sales per customer for each store and day\n",
    "    #expanded['SalesPerCustomer']=expanded['Sales']/expanded['Customers']\n",
    "\n",
    "    df.set_index('Date', inplace=True)\n",
    "    df_test=df.copy()\n",
    "    print(df.shape)\n",
    "\n",
    "    ################################## create features on test data\n",
    "    \n",
    "    import datetime as dt\n",
    "    from datetime import datetime\n",
    "    test = pd.read_csv('data/test.csv', low_memory=False)\n",
    "    store = pd.read_csv('data/store.csv', low_memory=False)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    "    #%% 1. Merging store to train data\n",
    "\n",
    "    # =============================================================================\n",
    "    # 1.1 Since data has to have the same size as 'train', a container is created\n",
    "    # =============================================================================\n",
    "\n",
    "    # Creating dataframe\n",
    "    expanded_store = test\n",
    "\n",
    "    # =============================================================================\n",
    "    # 1.2 Merging variables which do not have to be changed\n",
    "    # =============================================================================\n",
    "\n",
    "    # Variables which can be merged right away\n",
    "    easy = store.loc[:,['Store','StoreType','Assortment','CompetitionDistance']]\n",
    "\n",
    "    # Variables which can be simply merged\n",
    "    expanded_store = pd.merge(expanded_store, easy, on=['Store'])\n",
    "\n",
    "    #%% 2. Creating a dummy variable since for competition is open for every store\n",
    "\n",
    "    # =============================================================================\n",
    "    # 2.1 Checking whether there is actually competition\n",
    "    # =============================================================================\n",
    "\n",
    "    # Dataset of the variables which have to be transformed\n",
    "    competition = store.loc[:,['Store',\n",
    "                               'CompetitionOpenSinceMonth','CompetitionOpenSinceYear']]\n",
    "\n",
    "    # For easier looping\n",
    "    competition.set_index('Store', inplace=True)\n",
    "\n",
    "    # Finding stores which have competition openend\n",
    "    open_comp = competition.any(axis=1)\n",
    "\n",
    "    # Stores with competition\n",
    "    open_stores = competition.loc[open_comp].index.unique()\n",
    "\n",
    "    # =============================================================================\n",
    "    # 2.2 Generating dummy for the time competition is existing\n",
    "    # =============================================================================\n",
    "\n",
    "    # Generating Date when opened\n",
    "    for store_id in open_stores:\n",
    "        year = competition.loc[store_id,'CompetitionOpenSinceYear'].astype(int)\n",
    "        month = competition.loc[store_id,'CompetitionOpenSinceMonth'].astype(int)\n",
    "        competition.loc[store_id,'CompetitionDate'] = datetime(year,month,1)\n",
    "\n",
    "    # Creating a dummy variable for whether competition openend for each store\n",
    "    expanded_store.loc[:,'CompetitionOpened'] = 0\n",
    "\n",
    "    for store_id in open_stores:\n",
    "\n",
    "        # Getting the date when competition openend\n",
    "        date = competition.loc[store_id,'CompetitionDate']\n",
    "        date_str= date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        store_number = expanded_store.loc[:,'Store'] == store_id\n",
    "\n",
    "        maximum_date = expanded_store.loc[store_number,'Date'].max()\n",
    "\n",
    "        # Whether it falls in time frame\n",
    "        competition_existing = (expanded_store.loc[:,'Date'].between(date_str,maximum_date)) &  (expanded_store.loc[:,'Store'] == store_id)\n",
    "\n",
    "        # Indicating whether competition is around\n",
    "        expanded_store.loc[competition_existing, 'CompetitionOpened'] = 1\n",
    "\n",
    "\n",
    "    #%% 3. Creating a dummy variable for Promo\n",
    "\n",
    "    # =============================================================================\n",
    "    # 3.1 Since data has to have the same size as 'train', a container is created\n",
    "    # =============================================================================\n",
    "\n",
    "    # Create dataframe\n",
    "    promo2 = store.loc[:,['Store',\n",
    "                         'Promo2','Promo2SinceWeek','Promo2SinceYear','PromoInterval']]\n",
    "\n",
    "    # =============================================================================\n",
    "    # 3.2 Create dummy for whether a promo2 is running\n",
    "    # =============================================================================\n",
    "\n",
    "    # Getting date from which promo started\n",
    "    length = promo2.shape[0]\n",
    "    for i in range(length):\n",
    "        if promo2.loc[i,'Promo2']:\n",
    "            week = promo2.loc[i,'Promo2SinceWeek'].astype(int)\n",
    "            year = promo2.loc[i,'Promo2SinceYear'].astype(int)\n",
    "            promo2.loc[i,'promo2start'] = dt.datetime.strptime(f'{year}-W{int(week )- 1}-1', \"%Y-W%W-%w\").date()\n",
    "\n",
    "    # Merge it with the train file\n",
    "    expanded_promo = pd.merge(expanded_store, promo2, on=['Store'])\n",
    "\n",
    "\n",
    "    # Empty container with no promo indicator\n",
    "    expanded_promo.loc[:,'Promo2GoingOn'] = 0\n",
    "\n",
    "    # Month indication\n",
    "    expanded_promo.loc[:,'Date_str'] = pd.to_datetime(expanded_promo.loc[:,'Date'],).dt.strftime('%Y-%b-%d')\n",
    "    expanded_promo.loc[:,'Date'] = pd.to_datetime(expanded_promo.loc[:,'Date'],)\n",
    "    expanded_promo.loc[:,'month'] = expanded_promo.loc[:,'Date_str'].str[5:8]\n",
    "\n",
    "\n",
    "    months = expanded_promo.loc[:,'month'].unique()\n",
    "\n",
    "    for month in months:\n",
    "        month_boolean = expanded_promo.loc[:,'PromoInterval'].str.contains(month, na=False)\n",
    "        expanded_promo.loc[month_boolean,'Promo2GoingOn'] = 1\n",
    "\n",
    "\n",
    "    #%% 4. Creating time since competition opened\n",
    "\n",
    "    exp_comp = pd.merge(expanded_promo, competition, on=['Store'])\n",
    "\n",
    "    date_current = pd.to_datetime(exp_comp.loc[:,'Date'])\n",
    "    date_openend = pd.to_datetime(exp_comp.loc[:,'CompetitionDate'])\n",
    "\n",
    "    exp_comp.loc[:,'comp_open_since'] = (date_current - date_openend).astype('timedelta64[D]')\n",
    "\n",
    "    future_comp = (exp_comp.loc[:,'comp_open_since'] < 0)\n",
    "    no_comp = exp_comp.loc[:,'comp_open_since'].isna()\n",
    "\n",
    "    exp_comp.loc[future_comp, 'comp_open_since'] = 0\n",
    "    exp_comp.loc[no_comp, 'comp_open_since'] = 0\n",
    "\n",
    "    expanded_promo.loc[:,'comp_open_since'] = exp_comp.loc[:,'comp_open_since']\n",
    "\n",
    "    #%% 5. Decaying competition factor\n",
    "\n",
    "    expanded_promo.loc[:,'IntervalList'] = expanded_promo.loc[:,'PromoInterval'].str.split(pat = ',')\n",
    "\n",
    "    Interval = {'First': 0,\n",
    "    'Second' : 1,\n",
    "    'Third' : 2,\n",
    "    'Fourth': 3}\n",
    "\n",
    "    for element, value in Interval.items():\n",
    "        expanded_promo.loc[:,element] = expanded_promo.loc[:,'IntervalList'].str[value]\n",
    "\n",
    "    year = pd.to_datetime(expanded_promo.loc[:,'Date']).dt.year.astype(str)\n",
    "\n",
    "    Interval = {'Interval1': 'First',\n",
    "    'Interval2' : 'Second',\n",
    "    'Interval3' : 'Third',\n",
    "    'Interval4': 'Fourth'}\n",
    "\n",
    "    expanded_promo.loc[:,'Date_Actual'] = pd.to_datetime(expanded_promo.loc[:,'Date'])\n",
    "\n",
    "    for element, value in Interval.items():\n",
    "\n",
    "        ### New year stuff\n",
    "        expanded_promo.loc[:,'Date_Str'] = '1' + '-' + expanded_promo.loc[:,value] + '-' + year\n",
    "        dates = pd.to_datetime(expanded_promo.loc[:,'Date_Str'])\n",
    "\n",
    "        expanded_promo.loc[:,element] = (expanded_promo.loc[:,'Date_Actual'] - dates).astype('timedelta64[D]')\n",
    "\n",
    "        negative = expanded_promo.loc[:,element] < 0\n",
    "        expanded_promo.loc[negative,element] = np.nan\n",
    "\n",
    "        ### Last year stuff\n",
    "        expanded_promo.loc[:,'Date_Str'] = '1' + '-' + expanded_promo.loc[:,value] + '-' + (year.astype(int)-1).astype(str)\n",
    "        dates = pd.to_datetime(expanded_promo.loc[:,'Date_Str'])\n",
    "\n",
    "        expanded_promo.loc[:,element + 'before'] = (expanded_promo.loc[:,'Date_Actual'] - dates).astype('timedelta64[D]')\n",
    "\n",
    "        negative = expanded_promo.loc[:,element + 'before'] < 0\n",
    "        expanded_promo.loc[negative,element + 'before'] = np.nan\n",
    "\n",
    "    all_versions = expanded_promo.loc[:,['Interval1','Interval2','Interval3','Interval4',\n",
    "                                         'Interval1before','Interval2before','Interval3before','Interval4before',]]\n",
    "    minimum_distance = all_versions.min(axis=1, skipna=True)\n",
    "\n",
    "    expanded_promo.loc[:,'DaysFromPromotion'] = minimum_distance\n",
    "\n",
    "    expanded_promo.loc[:,'Decay'] = np.exp(- 0.05 * minimum_distance)\n",
    "\n",
    "    expanded_promo=expanded_promo[['Date', 'Store','Customers', 'DayOfWeek', 'Open', 'Promo','StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment','CompetitionDistance', 'CompetitionOpened', 'Promo2', 'Promo2SinceWeek','Promo2SinceYear', 'Promo2GoingOn', 'comp_open_since','Sales', 'DaysFromPromotion','Decay']]\n",
    "    expanded=expanded_promo.copy()\n",
    "    expanded_promo.info()\n",
    "\n",
    "    #%% Rachel - General stuff\n",
    "\n",
    "    # Convert Dates to Date time\n",
    "    expanded=expanded_promo.copy()\n",
    "\n",
    "    expanded['Date']=pd.to_datetime(expanded['Date'])\n",
    "\n",
    "    #add in variables for day of month etc\n",
    "    expanded['dayofweek'] = expanded['Date'].dt.dayofweek\n",
    "    expanded['quarter'] = expanded['Date'].dt.quarter\n",
    "    expanded['month'] = expanded['Date'].dt.month\n",
    "    expanded['year'] = expanded['Date'].dt.year\n",
    "    expanded['dayofyear'] = expanded['Date'].dt.dayofyear\n",
    "    expanded['dayofmonth'] = expanded['Date'].dt.day\n",
    "    expanded['weekofyear'] = expanded['Date'].dt.weekofyear\n",
    "\n",
    "    #create dummy variables for day of week etc and categorical variables\n",
    "\n",
    "    expanded= pd.get_dummies(expanded,columns=['dayofweek','dayofmonth','quarter','month','StateHoliday','StoreType','Assortment'])\n",
    "\n",
    "    #Re-add original day of month variable etc.\n",
    "    expanded['dayofweek'] = expanded['Date'].dt.dayofweek\n",
    "    expanded['quarter'] = expanded['Date'].dt.quarter\n",
    "    expanded['month'] = expanded['Date'].dt.month\n",
    "    expanded['year'] = expanded['Date'].dt.year\n",
    "    expanded['dayofyear'] = expanded['Date'].dt.dayofyear\n",
    "    expanded['dayofmonth'] = expanded['Date'].dt.day\n",
    "    expanded['weekofyear'] = expanded['Date'].dt.weekofyear\n",
    "\n",
    "    expanded.dropna(axis = 0, how ='any',inplace=True)\n",
    "    expanded=expanded[expanded['Sales'] >0 ]\n",
    "    \n",
    "    \n",
    "    print(expanded.shape)\n",
    "\n",
    "    #%% Rachel - Merge on relevant variables\n",
    "\n",
    "    # =============================================================================\n",
    "    # Extracting relevant variables with the relevant merge key variables\n",
    "    # =============================================================================\n",
    "\n",
    "    SalesPerCustomer_df = df_test.loc[:,['av_SalesPerCustomer','Store']].drop_duplicates()\n",
    "    SalesPerCustomer_dayW  = df_test.loc[:,['av_SalesPerCustomer_dayofweek','Store','dayofweek']].drop_duplicates()\n",
    "    SalesPerCustomer_dayM = df_test.loc[:,['av_SalesPerCustomer_dayofmonth','Store','dayofmonth']].drop_duplicates()\n",
    "\n",
    "    # =============================================================================\n",
    "    # Merging the relevant variables to the main dataset\n",
    "    # =============================================================================\n",
    "\n",
    "    test_merge = pd.merge(expanded,SalesPerCustomer_df, on=['Store'], validate='many_to_one')\n",
    "    test_merge = pd.merge(test_merge, SalesPerCustomer_dayW, on=['Store','dayofweek'], validate='many_to_one')\n",
    "    test_merge = pd.merge(test_merge, SalesPerCustomer_dayM, on=['Store','dayofmonth'], validate='many_to_one')\n",
    "\n",
    "    y_final_test=test_merge['Sales']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSPE (test): 5.959844%\n",
      "RMSPE (test): 5.959844%\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "test_columns=['av_SalesPerCustomer','av_SalesPerCustomer_dayofweek',\n",
    "                               'av_SalesPerCustomer_dayofmonth','Customers',\n",
    "           'Promo','Promo2','CompetitionDistance','dayofweek','Decay','comp_open_since']\n",
    "test_merge=test_merge[test_columns]\n",
    "X_final_test=test_merge.copy()\n",
    "\n",
    "    \n",
    "#params1=pd.read_csv('data/params.csv')\n",
    "xg_reg2 = xgb.XGBRegressor(**params1,n_estimators=500)\n",
    "X_final_test=test_merge.copy()\n",
    "    #####calculate test statistics\n",
    "    \n",
    "    \n",
    "EPSILON = 1e-10\n",
    "def _error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Simple error \"\"\"\n",
    "    return actual - predicted\n",
    "def _percentage_error(actual: np.ndarray, predicted: np.ndarray):\n",
    "\n",
    "    return _error(actual, predicted) / (actual + EPSILON)\n",
    "\n",
    "def rmspe(actual: np.ndarray, predicted: np.ndarray):\n",
    "\n",
    "    return np.sqrt(np.mean(np.square(_percentage_error(actual, predicted))))\n",
    "    \n",
    "final_test_preds = xg_reg.predict(X_final_test)\n",
    "print(\"RMSPE (test): %f\" % (rmspe(y_final_test,final_test_preds)*100) +'%')\n",
    "print(\"RMSPE (test): %f\" % (rmspe(y_final_test,final_test_preds)*100) +'%')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         3697.0\n",
       "1         4369.0\n",
       "2         7069.0\n",
       "3         5657.0\n",
       "4         5824.0\n",
       "           ...  \n",
       "216180    2973.0\n",
       "216181    3916.0\n",
       "216182    6995.0\n",
       "216183    6446.0\n",
       "216184    3366.0\n",
       "Name: Sales, Length: 216185, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_final_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2013-01-02    3697.0\n",
       "2013-10-02    4369.0\n",
       "2013-05-02    7069.0\n",
       "2014-01-02    5657.0\n",
       "2013-08-02    5824.0\n",
       "               ...  \n",
       "2013-07-29    8416.0\n",
       "2013-01-29    2973.0\n",
       "2013-10-29    3916.0\n",
       "2013-05-29    6446.0\n",
       "2014-01-29    3366.0\n",
       "Name: Sales, Length: 176068, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216185, 89)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"data/params.txt\",\"w\")\n",
    "f.write(str(params1))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "with open(\"data/params.txt\", \"r\") as data:\n",
    "    dictionary = ast.literal_eval(data.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.7978421458577196,\n",
       " 'lambd': 2.5982975135904516,\n",
       " 'max_depth': 4,\n",
       " 'subsample': 0.8913639174845348}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSPE (test): 5.959844%\n",
      "RMSPE (test): 5.959844%\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "test_columns=['av_SalesPerCustomer','av_SalesPerCustomer_dayofweek',\n",
    "                               'av_SalesPerCustomer_dayofmonth','Customers',\n",
    "           'Promo','Promo2','CompetitionDistance','dayofweek','Decay','comp_open_since']\n",
    "test_merge=test_merge[test_columns]\n",
    "X_final_test=test_merge.copy()\n",
    "\n",
    "    \n",
    "\n",
    "import ast\n",
    "with open(\"data/params.txt\", \"r\") as data:\n",
    "    dictionary = ast.literal_eval(data.read())    \n",
    "\n",
    "params1=dictionary\n",
    "xg_reg2 = xgb.XGBRegressor(**params1,n_estimators=500)\n",
    "X_final_test=test_merge.copy()\n",
    "    #####calculate test statistics\n",
    "    \n",
    "    \n",
    "EPSILON = 1e-10\n",
    "def _error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Simple error \"\"\"\n",
    "    return actual - predicted\n",
    "def _percentage_error(actual: np.ndarray, predicted: np.ndarray):\n",
    "\n",
    "    return _error(actual, predicted) / (actual + EPSILON)\n",
    "\n",
    "def rmspe(actual: np.ndarray, predicted: np.ndarray):\n",
    "\n",
    "    return np.sqrt(np.mean(np.square(_percentage_error(actual, predicted))))\n",
    "    \n",
    "final_test_preds = xg_reg.predict(X_final_test)\n",
    "print(\"RMSPE (test): %f\" % (rmspe(y_final_test,final_test_preds)*100) +'%')\n",
    "print(\"RMSPE (test): %f\" % (rmspe(y_final_test,final_test_preds)*100) +'%')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
